
##### Algebra Lineare
$f:R \to R$ si legge "$f$ definita in $R$ a valori in $R$".

---
##### Strutture Algebriche
- **Insieme**: collezione di un elementi che hanno tutti una stessa caratteristica
- **Funzione**: dati due insiemi $A$ e $B$, si dice $f$ (funzione) una legge che associa ad ogni elemento di $A$ uno di $B$. [[01_Parte1#Funzioni |Spiegazione approfondita delle funzioni]]
---
##### Matrici

**Matrice**: una matrice Ã¨ una tabella di **m-righe e n-colonne**, ogni singola "cella" di questa matrice si chiama **entrata**.
Una matrice con lo stesso numeri di righe e di colonne si dice **quadrata**, queste tipo di matrici possono assumere varie forme:
- **Triangolare diagonale**: se sopra **e** sotto la diagonale principale ci sono tutti zero.
- **Triangolare superiore**: se sotto la diagonale principale ci sono tutti zero.
- **Triangolare inferiore**: se sopra la diagonale principale ci sono tutti zero.
Una qualsiasi matrice si puÃ² dire:
- **Simmetrica**: se la matrice Ã¨ uguale alla sua trasposta.
- **Antisimmetrica**: se la matrice Ã¨ uguale all'opposta della sua trasposta.

**Matrice trasposta**: Ipotizzando una matrice A dove ogni entrata Ã¨ definita come $a_{ij}$ la matrice trasposta di A la matrice trasposta di A che denotiamo con $A^T$ con le entrate definite cosÃ¬: $a_{ji}$ 
![[Pasted image 20241116152010.png]]
Per trasporre una matrice basta scrivere le sue righe sotto forma di colonne. Il determinante di A Ã¨ uguale a determinante di $A^T$

**Matrici equivalenti**: 2 matrici sono uguali se ogni singola entrate Ã¨ uguale in entrambe le matrici

---
###### Operazioni tra matrici
**Somma tra matrici**: per sommare 2 matrici quest'ultime devono avere le stesse dimensioni
![[Pasted image 20241116152444.png]]
**Moltiplicazioni tra un numero e una matrice:** il numero per la quale moltiplichiamo la matrice viene detto scalare (nell'esempio sottostante lo scalare si chiama c)
![[Pasted image 20241116152547.png]]
**Sottrazione tra matrici:** per poter fare la differenza tra 2 matrici devo avere lo stesso numero di righe e di colonne in entrambe le matrici
**Moltiplicazioni tra matrici**: date 2 matrici A e B per poter fare la moltiplicazione tra queste deve essere vera **almeno una** delle 2 condizioni sottostanti:
1. numero di colonne di A  = numero di righe di B
2. numero di colonne di B = numero di righe di A
![[Pasted image 20241116153231.png]]
Ogni entrata della matrice "Risultato" Ã¨ il risultato di una moltiplicazione tra una riga della prima matrice e una colonna della seconda. Di seguito il calcolo dell'entrata in posizione 1,1 della matrice "Risultato":
![[Pasted image 20241116153434.png]]

---

**Matrice identitÃ :** denotiamo la matrice identitÃ  come $Id_m$. Definiamo matrice identitÃ  la matrice che rende vera questa cosa:
- $Id_m * A = A$
- $A * Id_m = A$
La matrice identitÃ  Ã¨ una matrice quadrata che segue questa forma
$$\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}
$$

**Forma di echelon:** una matrice si dice in forma di echelon se:
1. Se tutte le righe nulle sono in fondo
2. Se in ogni riga il primo elemento (detto **pivot**) $\not= 0$ partendo da sinistra si trova alla destra di tutti i pivot delle righe superiori e ha tutti zero sotto 
![[Pasted image 20241116155132.png]]
**Forma di echelon ridotta:** una matrice si dice in forma di echelon ridotta se:
1. Ã‰ in forma di echelon normale
2. tutti i pivot sono uguali ad 1
3. ogni pivot Ã¨ l'unico elemento non zero della sua colonna
![[Pasted image 20241116155409.png]]

**ğŸ˜Metodo di Gauss-Jordan:ğŸ˜** Questo metodo viene usato per trasformare qualsiasi matrice in forma di echelon ridotta, [[Metodo di Gauss-Jordan.pdf|Spiegazione di come funziona]] (farÃ² riferimento a questa tecnica con la notazione G-J).

---

**Rango**: il rango di una matrice Ã¨ il numero di pivot nella sua forma di echelon ridotta, il rango della matrice A lo denotiamo con $rkA$, questo numero rispetta sempre questa condizione:
	- $0 \leq rkA \leq min(m,n)$
	  Il rango di una matrice Ã¨ massimo $rkA = min(m,n)$

**InvertibilitÃ  delle matrici:** una matrice si dice invertibile se il suo rango Ã¨ massimo, denotiamo l'inversa della matrice $A$ con $A^{-1}$ 
**Inversa di una matrice**: Data una matrice A per trovare la sua inversa seguo questi step:
1. Controllo se il suo rango Ã¨ massimo
2. Creo una nuova matrice unendo la matrice A e la matrice identitÃ 
   ![[Pasted image 20241116161255.png]]
3. Faccio Gauss-Jordan su questa matrice e ottengo una matrice di questo tipo
   ![[Pasted image 20241116161611.png]]
4. Se $D = id_m$ allora $D' = A^{-1}$  
![[Pasted image 20241116161811.png]]
Essendo $D = id_m$ allora $D' = B^{-1}$ 

---

**Sottomatrici**: Sia $a_{ij}âˆˆM_{n,n}(\mathbb{R})$ con $i,jâˆˆ{1,â€¦,n}$. La sottomatrice $A_{ij}$ di A Ã¨ una matrice ottenuta rimuovendo le i-esime e le j-esime colonne e righe.
![[Pasted image 20241116171341.png]]

---

**Minore**: Il termine "minore" in algebra lineare si riferisce a una sottomatrice di una matrice piÃ¹ grande. PiÃ¹ precisamente, il minore di una matrice Ã¨ il determinante di una sottomatrice ottenuta rimuovendo una o piÃ¹ righe e colonne dalla matrice originale. 

---

**Determinante**: Ã¨ numero associato ad ogni matrice che puÃ² dirci molto sulle proprietÃ  della matrice stessa. 

**Metodo di Sarrus per il calcolo del determinante nelle matrici quadrate** che consiste nei seguenti passaggi:
1. Prendi la tua matrice normale
   ![[Pasted image 20241116165902.png | 200px]]
2. Riscrivi le prime 2 colonne e tracci le diagonali
   ![[Pasted image 20241116165941.png]]
3. Faccio le moltiplicazioni delle diagonali, moltiplicando tutto per + nelle diagonali principali e per - nelle diagonali secondarie 
   

**Metodo di Laplace per il calcolo del determinante** : questa metodologia si basa su questa formula: 
- $\det(A) = \sum_{j=1}^{n} a_{ij}(-1)^{i+j} \cdot M_{ij}$ 
- meno generalizzata diventa: $\det A = a_{1 1} (-1)^{1+1} \left| A_{1 1} \right| + a_{1 2} (-1)^{1+2} \left| A_{1 2} \right| + \ldots + a_{1 m} (-1)^{1+m} \left| A_{1 m} \right|$
Dove:
- A Ã¨ la matrice $n \times n$.
- i Ã¨ l'indice della riga scelta per lo sviluppo.
- $a_{ij}$ Ã¨ l'elemento della matrice A alla posizione $(i,j)$.
- $M_{ij}$ Ã¨ il minore ottenuto eliminando la riga i e la colonna j dalla matrice A.
![[Pasted image 20241116174944.png]]

**Teorema di Binet:** se abbiamo 2 matrici quadrate A,B $\in$ $M_{n,n}(R)$ il $det(A*B) = det(A) * det(B)$
==Dimostrazione???==

**Teorema degli orlati:** Data una matrice A $\in M_{m,n}$ e una sottomatrice quadrata di $A$ detta $B$, con dimensioni $p*p$ se tutte le sotto matrici di A di dimensione $(p+1)(p+1)$ ottenute orlando $B$ hanno $det = 0$ allora $rkA = p = rkB$ (orlare una matrice consiste nell'aggiungere una riga o una colonna scelte arbitrariamente dalla matrice padre). Di seguito un'esempio con la seguente matrice:
![[Pasted image 20241116202537.png]]
1. Scegliamo una matrice con det $\not=$ 0
   ![[Pasted image 20241116202628.png]]
2. Alla matrice scelta aggiungiamo una riga ed una colonna scelte in modo casuale e ci calcoliamo il det
   ![[Pasted image 20241116202732.png]]
3. Aggiungiamo la riga 5 e la colonna 1 e calcoliamo il det
   ![[Pasted image 20241116202852.png]]
4. Continuando e provando notiamo che tutti i determinanti vengono uguali a 0 quindi il $rk A = p = rk B = 2$
---
- **Teorema Matrici Invertibili** dice che:
Â  Â  - a) una matrice A Ã¨ invertibile $\iff \det A \neq 0$
Â  Â  - b) se $\det A \neq 0$ allora la matrice inversa Ã¨: $A^{-1} = \frac{1}{\det A} \cdot A_{a} = \frac{1}{\det A} \cdot (A_{ij})^{T}$
Â  Â  - c) se $A$ Ã¨ invertibile allora $\det A^{-1} = \frac{1}{{\det A}}$
---
**==Continuare dalla lezione del 15 ottobre 2024 in specifico dai sistemi lineari==**
##### Spazi Vettoriali
- **Spazio Vettoriale**: si dice spazio vettoriale su un campo K (K-spazio vettoriale) un insieme su cui sono definite due operazioni + e * (G, + \*) e valgono le proprietÃ :
Â  Â  - (G, +) Ã¨ gruppo
Â  Â  - Con \* vale associativitÃ .
Â  Â  - Con \* esiste elemento neutro.
Â  Â  - Vale distributivitÃ  della somma rispetto al prodotto esterno. $(a+b)\cdot \vec v=a \vec v + b \vec v$
Â  Â  - Vale distributivitÃ  del prodotto esterno rispetto alla somma. $a \cdot (\vec v + \vec w)=a \vec v +a \vec w$
- **Sottospazio**: $W$ Ã¨ sottospazio di $V$ se $W \subseteq V$ e Â $W$ Ã¨ un $K$ spazio vettoriale rispetto alle operazioni di somma e prodotto definite su $V$.
- **Intersezione tra sottospazi**: Ã¨ sempre un sottospazio
- **Unione tra sottospazi**: Ã¨ sottospazio solo se uno dei due Ã¨ sottoinsieme dell'altro (ovvio)
- **Combinazione Lineare**: un vettore Ã¨ combinazione lineare di $\vec{v_{1}}, \vec{v_{2}}, \dots, \vec{v_{n}}$ se esistono $a_{1}, a_{2}, \dots, a_{n}$ tali che $v=a_{1}\vec{ v_{1}}+a_{2}\vec{ v_{2}}+\dots+a_{n}\vec{ v_{n}}$.
- **Insieme di Generatori**: dato un K-spazio vettoriale $V$, un insieme $(v_{1}, v_{2}, \dots, v_{n})$ Ã¨ detto insieme di generatori (indicato con $G \{ v_{1}, v_{2}, \dots, v_{n}\}$) se preso un qualunque vettore $\vec z \in V$ esso si puÃ² scrivere come combinazione lineare (C.L.) dei vettori di $G$.
- **Base**: un insieme Â $(v_{1}, v_{2}, \dots, v_{n})$ Ã¨ detto **Base** di $V$ se ogni elemento di $V$ Ã¨ combinazione lineare (C.L.) di Â $v_{1}, v_{2}, \dots, v_{n}$ in modo **unico**.
Â  Â  - $B$ Ã¨ base $\iff$ i vettori di $B$ sono L.I. e generatori.
- **Linearmente Indipendenti**: i vettori $v_{1}, v_{2}, \dots, v_{n}$ si dicono linearmente indipendenti se quando $a_{1}\vec{ v_{1}}+a_{2}\vec{ v_{2}}+\dots+a_{n}\vec{ v_{n}}=0$ allora ne deve seguire che $a_1=a_2=\dots=a_n=0$
- **Lemma di Steinitz**: numero di vettori generatori $\geq$ numero di vettori linearmente indipendenti.
- **Teorema che caratterizza una base**: dato $B = \{ v_{1}, v_{2}, \dots, v_{n} \}$, $B$ Ã¨ un insieme $\iff$ i vettori sono L.I. e generatori.
- **Teorema sulle Basi**: tutte le basi di un K-spazio vettoriale hanno lo stesso numero di elementi.
Â  Â  - **Dimostrazione** (2\*, L10): Si usa il Lemma di Steinitz.
---
##### Sistemi Lineari
- **Sistema Lineare**: sistema di equazioni a piÃ¹ incognite di massimo 1Â° grado (c'Ã¨ il termine noto)
- **Teorema di RouchÃ¨ Capelli NÂ°1**: $\rho(A) =\rho(A,B) \iff$ Sistema possibile (ammette almeno una soluzione).
- **Teorema di RouchÃ¨ Capelli NÂ° 2**: quando il sistema Ã¨ possibile ($\rho(A) =\rho(A,B) = \rho$) allora esistono $\infty^{n-\rho}$ soluzioni. $n-\rho$ indica il numero di incognite libere.
- **Teorema di Cramer**: $\det A \neq 0 \iff$ Sistema determinato (ammette una e una sola ($\exists!$) soluzione).
Â  Â  - L'unica soluzione si calcola con:
Â  Â  - $$\begin{cases}

x_{1}=\frac{\det B_{1}}{\det A} \\

x_{2}=\frac{\det B_{2}}{\det A} \\

\dots \\

x_{n}=\frac{\det B_{n}}{\det A} \\

\end{cases}$$

Â  Â  Â  Â  - $B_{1}$ si calcola sostituendo la $1^a$ colonna di $A$ con $B$.
Â  Â  Â  Â  - $B_{2}$ si calcola sostituendo la $2^a$ colonna di $A$ con $B$.
Â  Â  Â  Â  - $\dots$
Â  Â  Â  Â  - $B_{n}$ si calcola sostituendo la $n^a$ colonna di $A$ con $B$.
Â  Â  - **Dimostrazione** (3\*, L13):
Â  Â  Â  Â  - Nell'andata si dimostrano unicitÃ , esistenza e formula.
Â  Â  Â  Â  - L'unicitÃ  e l'esistenza si dimostrano con il **Teorema delle matrici invertibili**.
Â  Â  Â  Â  - La formula si dimostra svolgendo l'equazione dell'esistenza.
Â  Â  Â  Â  - Il ritorno si dimostra con il teorema di **RouchÃ¨ Capelli NÂ°2**.
- **Sistema lineare omogeneo**: sistema lineare con $B$ nullo (non esistono termini noti).
---
##### Applicazioni Lineari
- **Applicazione Lineare**: corrispondenza (funzione) tra due K-spazi vettoriali.
- **Immagine** ($\text{im} f$): insieme del codominio formato dai vettori immagine dei vettori del dominio.
Â  Â  - L'immagine Ã¨ sottospazio del codominio, si **dimostra** (4\*. L13) provando che Ã¨ chiusa rispetto alla somma e al prodotto esterno.
Â  Â  - **[Studio](immagine)**
Â  Â  Â  Â  - $\dim imf = \rho$
Â  Â  Â  Â  - **Base**: vettori L.I. di V (gli stessi che formano il rango)
Â  Â  Â  Â  - **Equazione Cartesiana**: metodo matrice Z (si mettono in riga i vettori base, nell'ultima riga le incognite, si calcola il determinante)
- **Nucleo** ($\ker f$): insieme del dominio formato dai vettori che hanno come immagine il vettore nullo.
Â  Â  - Il nucleo Ã¨ sottospazio del dominio, si **dimostra** (5\*, L14) provando che Ã¨ chiusa rispetto alla somma e al prodotto esterno esterno.
Â  Â  - **[Studio](nucleo)**
Â  Â  Â  Â  - $\dim \ker f = \dim V - \dim imf$
Â  Â  Â  Â  - **Base**: $A \cdot X=0$
Â  Â  Â  Â  - **Equazioni Cartesiane**: ricavate dal sistema precedente (o metodo matrice Z)

- **IniettivitÃ **: una funzione si dice iniettiva se presi due qualsiasi vettori del dominio diversi allora ne deve seguire che le loro immagini siano diverse.
- **SuriettivitÃ **: una funzione si dice suriettiva se ogni vettore del codominio Ã¨ raggiunto da almeno un vettore del dominio.
- **Teorema sul Nucleo e IniettivitÃ **: afferma che $f$ Ã¨ iniettiva $\iff \ker f = \{ 0 \}$
Â  Â  - **Dimostrazione** (6\*, L14)
- **Applicazione Identica**: applicazione lineare cui legge corrisponde a $i(\vec z) = \vec z$
- **Applicazione Inversa**: Date due applicazioni lineari $f: V \to W$ e $g: W \to V$ se $g \circ f = i_{v}$ e $f \circ g = i_{w}$ allora $f$ Ã¨ invertibile e $g$ Ã¨ detta applicazione inversa di $f \,\, (g=f^{-1})$.
Â  Â  - $f$ e $g$ devono essere suriettive e iniettive.
Â ---
##### Endomorfismi

- **Endomorfismo**: applicazione lineare dove dominio $=$ codominio.
- **Isomorfismo**: un'applicazione lineare biettiva (iniettiva e suriettiva), quindi necessariamente endomorfismo.
- **Autovalore**: dato un endomorfismo $f: V \to V$, $\lambda$ si dice **autovalore** se esiste un vettore $v \in V$ con $v \neq 0$ tale che $f(v)=\lambda v$. $\qquad\lambda \space\text{autovalore} \iff \exists v \in V, v \neq 0 \space|\space f(v) = \lambda v$
- **Autovettore**: dato un endomorfismo $f: V \to V$, $v \in V$, $v \neq 0$ si dice **autovettore** se esiste un $\lambda \in K$ tale che $f(v) = \lambda v$. $\qquad v \in V, v \neq 0 \space\text{autovettore} \iff \exists \lambda \in K \space|\space f(v)=\lambda v$
- **Autospazio**: dato un endomorfismo $f: V \to V$, si dice **autospazio** $V_{\lambda}$ il sottospazio di V definito nel modo seguente: $\quad V_{\lambda} = \{ v \in V \space|\space f(v)=\lambda v \} \subseteq V$
- **Polinomio Caratteristico**: data una matrice $A$ il $P.C. = \det{(A-T\cdot I)}$.
- **MolteplicitÃ  Algebrica**: per molteplicitÃ  algebrica di $\lambda$ si intende il numero di volte in cui $\lambda$ Ã¨ soluzione del polinomio caratteristico.
- **MolteplicitÃ  Geometrica**: per molteplicitÃ  geometrica di $\lambda$ si intende la dimensione dell'autospazio $V_{\lambda}$.
- **Endomorfismo Associato all'Autovalore**: indichiamo con $f_{\lambda}$ l'endomorfismo associato all'autovalore $\lambda$. $f_{\lambda}(v)=f(v)-\lambda v$
- **Teorema sulle MolteplicitÃ **: dato un endomorfismo $f: V \to W$ e un autovalore $\lambda \in K$ allora $0 < g_{\lambda} \leq m_{\lambda}$.
- **Endomorfismo Semplice**: un endomorfismo si dice semplice se esiste una base formata interamente da autovettori.
- **Matrici simili**: due matrici $A$ e $B$ si dicono simili se Â $\exists \,P \in \mathbb{K}^{n,n} \,|\, P^{-1}AP=B$.
- **Teorema sulla diagonalizzazione**: una matrice $A \in \mathbb{K}^{n,n}$ Ã¨ diagonalizzabile $\iff$ $f_{A} : \mathbb{K}^{n} \to \mathbb{K}^{n}$ Ã¨ semplice oppure se Ã¨ simile a una matrice diagonale.
Â  Â  - **Matrice Diagonalizzata**: matrice che ha sulla diagonale principale le molteplicitÃ  algebriche degli autovalori.
Â  Â  - **Matrice Diagonalizzante**: matrice che ha in colonna una base degli autovettori.
- **Teorema Autospazio**: sia $V$ un K-spazio vettoriale e $f: V \to W$ un endormofismo. Allora ne segue che $V_{\lambda} = \ker f_{\lambda}$.
Â  Â  - **Dimostrazione** (7\*, L19): si usa la definizione dell'autospazio.
Â ---
##### Geometria

---
